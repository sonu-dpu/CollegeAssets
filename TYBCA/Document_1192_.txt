     Practical Assignment – 6
1. Write a python program to implement k-means algorithm to build prediction model (Use Credit Card Dataset CC GENERAL.csv Download from kaggle.com)
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

# Load data
df = pd.read_csv("CC GENERAL.csv")

# Drop ID column
df = df.drop(columns=["CUST_ID"])

# Fill missing values with median
imputer = SimpleImputer(strategy="median")
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Scale features to 0-1 range
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df.columns)

# Run KMeans with 5 clusters (you can change this number)
kmeans = KMeans(n_clusters=5, random_state=42)
df_scaled["Cluster"] = kmeans.fit_predict(df_scaled)

# Add cluster labels back to original data (optional)
df["Cluster"] = df_scaled["Cluster"]

# Show count of points in each cluster
print(df["Cluster"].value_counts())
2. Write a python program to implement hierarchical Agglomerative clustering algorithm. (Download Customer.csv dataset from github.com).
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Load dataset
df = pd.read_csv("Customers.csv")

# Preview dataset (optional)
print(df.head())

# Drop non-numeric or ID columns if any (adjust column names accordingly)
# For example, if there's 'CustomerID' or 'Name' column, drop them:
df_numeric = df.select_dtypes(include=[np.number])  # keep only numeric columns

# Handle missing values if any (simple median imputation)
df_numeric = df_numeric.fillna(df_numeric.median())

# Scale features 0-1
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df_numeric)

# Plot dendrogram to help decide number of clusters
linked = linkage(data_scaled, method='ward')

plt.figure(figsize=(10, 7))
dendrogram(linked,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=False)
plt.title('Dendrogram')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()

# Choose number of clusters, e.g. 3
n_clusters = 3
agg_clust = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')
labels = agg_clust.fit_predict(data_scaled)

# Add cluster labels back to original dataframe
df['Cluster'] = labels

# Print cluster counts
print(df['Cluster'].value_counts())

# Optional: save to CSV
df.to_csv("Customer_with_clusters.csv", index=False)
print("Clustering complete. Results saved to Customer_with_clusters.csv")
3. Write a python program to implement k-means algorithms on a synthetic dataset.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# **Step 1: Generate Synthetic Dataset**
n_samples = 300
n_features = 2
n_clusters = 4
random_state = 42

X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=random_state)

# **Step 2: Visualize the Dataset**
plt.scatter(X[:, 0], X[:, 1], c='gray', s=30)
plt.title("Synthetic Dataset")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# **Step 3: Apply K-Means Algorithm**
kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
kmeans.fit(X)

# **Step 4: Visualize Clusters**
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=30)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("K-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()
4. Write a python program to implement hierarchical clustering algorithm. (Download Wholesale customers data dataset from github.com).
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# Step 1: Load the dataset
url = "Wholesale customers data.csv"
df = pd.read_csv(url)

# Step 2: Drop non-numeric or categorical columns if needed
data = df.drop(columns=['Channel', 'Region'])

# Step 3: Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Step 4: Perform Hierarchical Clustering
linked = linkage(scaled_data, method='ward')  # You can try 'single', 'complete', etc.

# Step 5: Plot the dendrogram
plt.figure(figsize=(10, 5))
dendrogram(linked, truncate_mode='lastp', p=10)
plt.title('Hierarchical Clustering Dendrogram (last 10 merges)')
plt.xlabel('Sample index or cluster size')
plt.ylabel('Distance')
plt.show()

# Step 6: Create clusters
num_clusters = 4
cluster_labels = fcluster(linked, num_clusters, criterion='maxclust')

# Step 7: Add cluster labels to DataFrame
df['Cluster'] = cluster_labels
print(df[['Cluster']].value_counts())

# Step 8: Plot clusters using two original features (e.g., 'Grocery' vs 'Milk')
plt.figure(figsize=(8, 6))
plt.scatter(data['Grocery'], data['Milk'], c=cluster_labels, cmap='tab10', s=50)
plt.title('Clusters based on Grocery and Milk')
plt.xlabel('Grocery')
plt.ylabel('Milk')
plt.grid(True)
plt.show()
